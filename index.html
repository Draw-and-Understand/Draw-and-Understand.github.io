<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
  <link rel="icon" type="image/x-icon" href="static/images/sphinx-v-logo.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>

<!-- Paper Title and Author -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Weifeng Lin</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="">Xinyu Wei</a><sup>1</sup>,
            </span>
            <!-- <span class="author-block">
              <a href="https://jonbarron.info">Jonathan T. Barron</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://sofienbouaziz.com">Sofien Bouaziz</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com">Dan B Goldman</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a><sup>2</sup>
            </span> -->
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shanghai AI Lab,</span>
            <span class="author-block"><sup>2</sup>Peking University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Gradio Link. -->
              <span class="link-block">
                  <a href="https://665adc2845309519e8.gradio.live/"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <img src="static/images/gradio-icon-seeklogo.svg" alt="Gradio Logo">
                      </span>
                      <span>Demo</span>
                  </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline width="200%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into free-viewpoint portraits.
      </h2>
    </div>
  </div>
</section> -->

<!-- Rolling Banner for examples -->
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper Abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The interaction between humans and artificial intelligence (AI) is a crucial factor that 
            reflects the effectiveness of multimodal large language models (MLLMs). 
            However, current MLLMs primarily focus on image-level comprehension and limit 
            interaction to textual instructions, thereby constraining their flexibility in usage 
            and depth of response. In this paper, we introduce the Draw-and-Understand project: 
            <strong>a new model</strong>, <strong>a multi-domain dataset</strong>, and <strong>a challenging benchmark</strong> for visual prompting. 
          </p>
          <p>
            Specifically, we propose <span class="sphinx-v">SPHINX-V</span>, a new end-to-end trained Multimodal Large Language Model (MLLM) 
            that connects a vision encoder, a visual prompt encoder and an LLM for various visual prompts 
            (points, bounding boxes, cycle and free-form shape) and language understanding. 
            To advance visual prompting research for MLLMs, we introduce <span class='mdvp-data'>MDVP-Data</span> and MDVP-Bench. 
            <span class='mdvp-data'>MDVP-Data</span> features a multi-domain dataset containing 1.2M unique image-visual prompt-text 
            instruction-following samples, including natural images, document images, OCR images, 
            mobile screenshots, web screenshots, and multi-panel images. Furthermore, we present MDVP-Bench, 
            a comprehensive and challenging benchmark to assess a model's capability in understanding visual 
            prompting instructions.
          </p>
          <p>
            Our experiments demonstrate <span class="sphinx-v">SPHINX-V</span>'s impressive multimodal interaction capabilities through 
            visual prompting, revealing significant improvements in detailed region description and 
            question-answering abilities.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Pipeline -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-seven-fifths">
        <h2 class="title is-3">
          <img src="static/images/sphinx-v-logo.png" alt="Icon" style="height: 1.1em; vertical-align: middle; margin-right: 0.1em;">
          <span class="sphinx-v">SPHINX-V</span>: Understanding Arbitrary Format Visual Prompts and Images from Diverse Domains
        </h2>
        <div class="content has-text-justified">
          <p>
            <span class="sphinx-v">SPHINX-V</span> demonstrates robust capabilities in recognizing diverse visual prompts and images 
            across various domains, enabling execution of complex tasks such as Visual Question Answering (VQA), 
            Optical Character Recognition (OCR), Regional Captioning, and other intricate referring tasks. 
            This formidable proficiency stems from its simple yet effective model architecture and training 
            strategies:
          </p>
          <div style="text-align: center;">
            <img src="static/images/arch-1.png" alt="arch" style="height: auto; width: 100%; display: inline-block;">
          </div>
          <p></p>
          <p>
            <span class="sphinx-v">SPHINX-V</span> comprises three components: a mixed vision encoder, 
            a versatile visual prompt encoder, and a large language model (LLM). 
            Given an image alongside coordinates of specific points or regions and input 
            instruction language, we perform tokenization and encoding to derive three distinct embeddings: 
            image embeddings <span class="tex">\( Z_i \)</span>, visual prompt embeddings <span class="tex">\( Z_v \)</span>, 
            and text embeddings <span class="tex">\( Z_t \)</span>.
            Subsequently, we employ two projection matrixs to convert <span class="tex">\( Z_i \)</span> and 
            <span class="tex">\( Z_v \)</span> into 
            language embedding tokens, aligning them with the word embedding space in the language model. 
            Finally, we integrate the image, visual prompt, and language representations, 
            forwarding them collectively to the LLM to attain pixel/region-level understanding capability.
          </p>
          <p><span class="sphinx-v">SPHINX-V</span> is trained in two stages:</p>

              <ul>
                <li><strong>Stage 1: Image-Visual Prompt-Text Alignment Pre-training.</strong>
                  <p>By employing a visual prompt encoder, we initially freeze both the pre-trained vision encoder and the pre-trained LLM. We focus on training the features of visual prompts to align with the features of image-visual prompt-text. We implement an MLP as the connector between visual prompts and language to enhance the multimodal prompting capabilities of the model.</p>
                </li>
                
                <li><strong>Stage 2: Multi-Task Supervised Finetuning.</strong>
                  <p>At this stage, we load the weights trained during stage 1 and keep the vision encoder and visual prompt encoder weights frozen. We then fine-tune the visual prompt projector and the LLM model. Our focus is on enhancing <span class="sphinx-v">SPHINX-V</span>'s ability to accurately interpret user instructions and handle diverse pixel-level understanding tasks, such as detailed captioning, inter-relationship analysis, and complex reasoning.</p>
                </li>
              </ul>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- MDVP Dataset -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">
          <img src="static/images/database.png" alt="Icon" style="height: 1.2em; vertical-align: middle; margin-right: 0.1em;">
          <span class='mdvp-data'>MDVP-Data</span>: Multi-domain Visual-Prompt Instruction Dataset
        </h2>
        <div class="content has-text-justified">
          <p>We introduce <span class="mdvp-data">MDVP-Data</span>, an instruction dataset designed to foster 
            fine-grained and open-world image understanding in MLLMs, encompassing approximately 1.2 million 
            multimodal dialogues. <span class="mdvp-data">MDVP-Data</span> integrates both point-level 
            and region-level instruction data derived from public datasets. It consists of two types of data:
          </p>
            <ol>
              <li>Restructured public grounding datasets formatted for visual prompt-based instruction following tuning;</li>
              <li>High-quality training pairs developed using meticulously crafted prompt templates, produced through the GPT-4V model.</li>
            </ol>
            <div style="text-align: center;">
              <img src="static/images/data_fig-1.png" alt="arch" style="height: auto; width: 100%; display: inline-block;">
            </div>
            <p></p>
            <p>
              Above diagram displays the distribution of images drawn from various sources, including nature scenes, 
              OCR texts, web content, mobile interfaces, documents, and multi-pane graphics. 
              It also features a sample from the GPT-assisted MDVP dataset, emphasizing the diversity 
              and richness of its point-based and region-based instruction-following data.
            </p>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- Performance -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">
          <img src="static/images/performance.png" alt="Icon" style="height: 1.4em; vertical-align: middle; margin-right: 0.1em;">
          Performance
        </h2>
        <h3 class="title is-4" style="text-align: left;">
          <img src="static/images/tradition-bench.png" alt="Icon" style="height: 1.2em; vertical-align: middle; margin-right: 0.1em;">
          Traditional Evaluating Task
        </h3>
        <div class="content has-text-justified">
          <p>
            In traditional evaluation tasks, <span class="sphinx-v">SPHINX-V</span> 
            significantly outperforms existing visual prompt-based methods:
          </p>
        </div>

        <h3 class="title is-4" style="text-align: left;">
          <img src="static/images/mdvp-bench.png" alt="Icon" style="height: 1.2em; vertical-align: middle; margin-right: 0.1em;">
          MDVP-Bench
        </h3>

        
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Natural Image -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Natural Image</h2>
          <p>
            Natural Image
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>

      <!-- Multi Panel Image -->
      <div class="column">
        <h2 class="title is-3">Multi Panel Image</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              Multi Panel Image
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>

    <div class="columns is-centered">

      <!-- Document -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Document</h2>
          <p>
            Document
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>

      <!-- Text Spot -->
      <div class="column">
        <h2 class="title is-3">Text Spot</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              Text Spot
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>

    </div>

    <div class="columns is-centered">

      <!-- Web Page -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Web Page</h2>
          <p>
            Web Page
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>

      <!-- Screen Shot -->
      <div class="column">
        <h2 class="title is-3">Screen Shot</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              Screen Shot
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>

    </div>


    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
