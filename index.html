<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
  <link rel="icon" type="image/x-icon" href="static/images/sphinx-v-logo.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>

<!-- Paper Title and Author -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block corresponding-author">
              <a href="">Weifeng Lin</a><sup>1</sup>,
            </span>
            <span class="author-block corresponding-author">
              <a href="">Xinyu Wei</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="">Ruichuan An</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="">Peng Gao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="">Bocheng Zou</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="">Yulin Luo</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="">Siyuan Huang</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="">Shanghang Zhang</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="">Hongsheng Li</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shanghai AI Lab,</span>
            <span class="author-block"><sup>2</sup>Peking University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Gradio Link. -->
              <span class="link-block">
                  <a href="https://665adc2845309519e8.gradio.live/"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <img src="static/images/gradio-icon-seeklogo.svg" alt="Gradio Logo">
                      </span>
                      <span>Demo</span>
                  </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Rolling Banner for examples -->
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel" style="flex-wrap: nowrap;">
        <div class="item example-2">
          <img src="static/images/example-2.png" alt="arch" style="height: auto; width: auto;">
        </div>
        <div class="item example-1">
          <img src="static/images/example-1.png" alt="arch" style="height: auto; width: auto;">
        </div>
        <div class="item example-3">
          <img src="static/images/example-3.png" alt="arch" style="height: auto; width: auto;">
        </div>
        <div class="item example-4">
          <img src="static/images/example-4.png" alt="arch" style="height: auto; width: auto;">
        </div>
        <div class="item example-5">
          <img src="static/images/example-5.png" alt="arch" style="height: auto; width: auto;">
        </div>
      </div>
      <p class="hero-centered-text"> ... slide to see more examples ...</p>
    </div>
  </div>
</section>
<script src="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.3/dist/js/bulma-carousel.min.js"></script>
		<script>
			bulmaCarousel.attach('#results-carousel', {
				slidesToScroll: 1,
				slidesToShow: 1,
        autoplay: true, 
        autoplaySpeed: 3000,
        loop: true
			});
</script>


<!-- Paper Abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The interaction between humans and artificial intelligence (AI) is a crucial factor that 
            reflects the effectiveness of multimodal large language models (MLLMs). 
            However, current MLLMs primarily focus on image-level comprehension and limit 
            interaction to textual instructions, thereby constraining their flexibility in usage 
            and depth of response. In this paper, we introduce the Draw-and-Understand project: 
            <strong>a new model</strong>, <strong>a multi-domain dataset</strong>, and <strong>a challenging benchmark</strong> for visual prompting. 
          </p>
          <p>
            Specifically, we propose <span class="sphinx-v">SPHINX-V</span>, a new end-to-end trained Multimodal Large Language Model (MLLM) 
            that connects a vision encoder, a visual prompt encoder and an LLM for various visual prompts 
            (points, bounding boxes, cycle and free-form shape) and language understanding. 
            To advance visual prompting research for MLLMs, we introduce <span class='mdvp-data'>MDVP-Data</span> and MDVP-Bench. 
            <span class='mdvp-data'>MDVP-Data</span> features a multi-domain dataset containing 1.2M unique image-visual prompt-text 
            instruction-following samples, including natural images, document images, OCR images, 
            mobile screenshots, web screenshots, and multi-panel images. Furthermore, we present MDVP-Bench, 
            a comprehensive and challenging benchmark to assess a model's capability in understanding visual 
            prompting instructions.
          </p>
          <p>
            Our experiments demonstrate <span class="sphinx-v">SPHINX-V</span>'s impressive multimodal interaction capabilities through 
            visual prompting, revealing significant improvements in detailed region description and 
            question-answering abilities.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Pipeline -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-seven-fifths">
        <h2 class="title is-3">
          <img src="static/images/sphinx-v-logo.png" alt="Icon" style="height: 1.1em; vertical-align: middle; margin-right: 0.1em;">
          <span class="sphinx-v">SPHINX-V</span>: Understanding Arbitrary Format Visual Prompts and Images from Diverse Domains
        </h2>
        <div class="content has-text-justified">
          <p>
            <span class="sphinx-v">SPHINX-V</span> demonstrates robust capabilities in recognizing diverse visual prompts and images 
            across various domains, enabling execution of complex tasks such as Visual Question Answering (VQA), 
            Optical Character Recognition (OCR), Regional Captioning, and other intricate referring tasks. 
            This formidable proficiency stems from its simple yet effective model architecture and training 
            strategies:
          </p>
          <div style="text-align: center;">
            <img src="static/images/arch-1.png" alt="arch" style="height: auto; width: 100%; display: inline-block;">
          </div>
          <p></p>
          <p>
            <span class="sphinx-v">SPHINX-V</span> comprises three components: a mixed vision encoder, 
            a versatile visual prompt encoder, and a large language model (LLM). 
            Given an image alongside coordinates of specific points or regions and input 
            instruction language, we perform tokenization and encoding to derive three distinct embeddings: 
            image embeddings <span class="tex">\( Z_i \)</span>, visual prompt embeddings <span class="tex">\( Z_v \)</span>, 
            and text embeddings <span class="tex">\( Z_t \)</span>.
            Subsequently, we employ two projection matrixs to convert <span class="tex">\( Z_i \)</span> and 
            <span class="tex">\( Z_v \)</span> into 
            language embedding tokens, aligning them with the word embedding space in the language model. 
            Finally, we integrate the image, visual prompt, and language representations, 
            forwarding them collectively to the LLM to attain pixel/region-level understanding capability.
          </p>
          <p><span class="sphinx-v">SPHINX-V</span> is trained in two stages:</p>

              <ul>
                <li><strong>Stage 1: Image-Visual Prompt-Text Alignment Pre-training.</strong>
                  <p>By employing a visual prompt encoder, we initially freeze both the pre-trained vision encoder and the pre-trained LLM. We focus on training the features of visual prompts to align with the features of image-visual prompt-text. We implement an MLP as the connector between visual prompts and language to enhance the multimodal prompting capabilities of the model.</p>
                </li>
                
                <li><strong>Stage 2: Multi-Task Supervised Finetuning.</strong>
                  <p>At this stage, we load the weights trained during stage 1 and keep the vision encoder and visual prompt encoder weights frozen. We then fine-tune the visual prompt projector and the LLM model. Our focus is on enhancing <span class="sphinx-v">SPHINX-V</span>'s ability to accurately interpret user instructions and handle diverse pixel-level understanding tasks, such as detailed captioning, inter-relationship analysis, and complex reasoning.</p>
                </li>
              </ul>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- MDVP Dataset -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">
          <img src="static/images/database.png" alt="Icon" style="height: 1.2em; vertical-align: middle; margin-right: 0.1em;">
          <span class='mdvp-data'>MDVP-Data</span>: Multi-domain Visual-Prompt Instruction Dataset
        </h2>
        <div class="content has-text-justified">
          <p>We introduce <span class="mdvp-data">MDVP-Data</span>, an instruction dataset designed to foster 
            fine-grained and open-world image understanding in MLLMs, encompassing approximately 1.2 million 
            multimodal dialogues. <span class="mdvp-data">MDVP-Data</span> integrates both point-level 
            and region-level instruction data derived from public datasets. It consists of two types of data:
          </p>
            <ol>
              <li>Restructured public grounding datasets formatted for visual prompt-based instruction following tuning;</li>
              <li>High-quality training pairs developed using meticulously crafted prompt templates, produced through the GPT-4V model.</li>
            </ol>
            <div style="text-align: center;">
              <img src="static/images/data_fig-1.png" alt="arch" style="height: auto; width: 100%; display: inline-block;">
            </div>
            <p></p>
            <p>
              Above diagram displays the distribution of images drawn from various sources, including nature scenes, 
              OCR texts, web content, mobile interfaces, documents, and multi-pane graphics. 
              It also features a sample from the GPT-assisted MDVP dataset, emphasizing the diversity 
              and richness of its point-based and region-based instruction-following data.
            </p>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- Performance -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">
          <img src="static/images/performance.png" alt="Icon" style="height: 1.4em; vertical-align: middle; margin-right: 0.1em;">
          Performance
        </h2>
        <h3 class="title is-4" style="text-align: left;">
          <img src="static/images/tradition-bench.png" alt="Icon" style="height: 1.2em; vertical-align: middle; margin-right: 0.1em;">
          Traditional Evaluating Task
        </h3>
        <p style="text-align: left; margin-bottom: 20px;">
          In traditional evaluation tasks, <span class="sphinx-v">SPHINX-V</span> significantly 
          outperforms existing visual-prompt-based methods.
        </p>
        <div class="content has-text-justified">
          <div class="image-row" style="margin-bottom: 20px;">
            <div class="image-column">
              <img src="static/images/ROC.png" alt="ROC" style="width: 95%;">
              <p>Semantic Similarity and Semantic IoU of ROC(referring object classification) on LVIS and PACO.</p>
            </div>
            <div class="image-column">
              <img src="static/images/OCR.png" alt="OCR" style="width: 95%;">
              <p>Accuracy of ROC on LVIS and OCR(optical character recognition) on COCO text.</p>
            </div>
          </div>
          <div class="image-row" style="margin-bottom: 20px;">
            <div class="image-column">
              <img src="static/images/detail-caption.png" alt="detail caption" style="width: 90%;">
              <p>Detailed region-level captioning performance on the RefCOCOg validation set. Using GPT-4V for evaluation.</p>
            </div>
            <div class="image-column">
              <img src="static/images/brief-caption.png" alt="brief caption" style="width: 90%;">
              <p>Brief region captioning performance evaluated on
                RefCOCOg. Using traditional evaluation metric.</p>
            </div>
          </div>
          <div style="text-align: center; margin-bottom: 20px;">
            <img src="static/images/VCR.png" alt="arch" style="height: auto; width: 60%; display: inline-block;">
            <p>Validation Accuracy on VCR dataset. 
              Q, A, R stands for Question, Answer, Rational, respectively.</p>
          </div>
        </div>

        
        


        <h3 class="title is-4" style="text-align: left;">
          <img src="static/images/mdvp-bench.png" alt="Icon" style="height: 1.2em; vertical-align: middle; margin-right: 0.1em;">
          MDVP-Bench
        </h3>
        <p style="text-align: left; margin-bottom: 20px;">
          To evaluate the proficiency of the MLLM in complex 
          pixel-level image understanding tasks and its versatility across various domains, 
          we initially curated a subset of our <span class="mdvp-data">MDVP-Data</span>. 
          This subset underwent a thorough 
          manual content review and filtering process, resulting in the creation of MDVP-Bench. 
          MDVP-Bench is a comprehensive and challenging benchmark covering a wide range of tasks, 
          including concise descriptions, elaborate narratives, analyses of interconnections among 
          different regions, and complex reasoning. The performance of existing visual-prompt-based 
          methods on MDVP-Bench is as follows:
        </p>
        <div style="text-align: center; margin-bottom: 20px;">
          <img src="static/images/mdvp-bench-table.png" alt="arch" style="height: auto; width: 80%; display: inline-block;">
        </div>

        
      </div>
    </div>
  </div>
</section>

<!-- MDVP domain example -->
<!-- <section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">


      <div class="column">
        <div class="content">
          <h2 class="title is-3">Natural Image</h2>
          <p>
            Natural Image
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>


      <div class="column">
        <h2 class="title is-3">Multi Panel Image</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              Multi Panel Image
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>

    <div class="columns is-centered">


      <div class="column">
        <div class="content">
          <h2 class="title is-3">Document</h2>
          <p>
            Document
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>


      <div class="column">
        <h2 class="title is-3">Text Spot</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              Text Spot
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>

    </div>

    <div class="columns is-centered">


      <div class="column">
        <div class="content">
          <h2 class="title is-3">Web Page</h2>
          <p>
            Web Page
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>


      <div class="column">
        <h2 class="title is-3">Screen Shot</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              Screen Shot
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>

    </div>


  </div>
</section> -->


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
